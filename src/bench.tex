\section{Benchmarks}
\label{sec:benchmarks}

This section describes the benchmarks we ran on transformed programs, to evaluate whether the transformation is worthwhile.

We executed all benchmarks on a laptop computer with an Intel |i7-4702HQ| quad-core processor with the frequency fixed at |2.2GHz|. The RAM memory available to the benchmarks was |2GB| of RAM. The benchmarks were ran either by executing the benchmark 1000 times and averaging the total time, for the first benchmark target, or by using the dedicated JHM benchmarking and performance analysis platform \cite{aleksey_shipilev_openjdk:_????} for the second benchmark target. We would like our readers to assume the numbers presented are preliminary. We plan to execute all benchmarks using JMH on a dedicated server machine.

We developed two transformations for the tests: the ADR plugin itself and an extension to the miniboxing plugin \cite{miniboxing} that transforms the function representation. We will present the benchmarks for each of these transformations individually.

\subsection{Ad hoc Data Representation Transformations}
\label{sec:benchmarks:ad-hoc}

We tested the |adrt| Scala compiler plugin (\S\ref{sec:impl}) using two different benchmarks:

\begin{compactitem}
\item the Gaussian integer greatest common divisor algorithm, presented in \S\ref{sec:problem};
\item the algorithm for computing the first 2000 Hamming numbers.
\end{compactitem}

For the Gaussian integer benchmark, we replaced the pair-based Gaussian integer representation by a long integer. Furthermore, we encoded all the operations, such as |+|, |-|, |/|, |%|
and others as extension methods:

\begin{lstlisting-nobreak}
adrt(IntPairComplexToLongComplex) {
  def gcd(n1: (Int, Int), n2: (Int, Int)): (Int, Int) = {
    val remainder = n1 % n2
    if (remainder.norm == 0) n2 else gcd(n2, remainder)
  }
}
\end{lstlisting-nobreak}

For the Hamming numbers benchmark, we took the code from the Rosetta Code repository\footnote{Available at: \url{http://rosettacode.org/wiki/Hamming_numbers#Scala}.} and transformed all reference to |BigInt| by |Long|:

\begin{lstlisting-nobreak}
adrt(QueueOfIntAsFunnyQueue) {
  class Hamming extends Iterator[Long] {
    import scala.collection.mutable.Queue
    val q2 = new Queue[Long]
    val q3 = new Queue[Long]
    val q5 = new Queue[Long]
    def enqueue(n: Long) =
      q2 enqueue n * 2; q3 enqueue n * 3; q5 enqueue n * 5
    def next = {
      val n = q2.head min q3.head min q5.head
      if (q2.head == n) q2.dequeue
      if (q3.head == n) q3.dequeue
      if (q5.head == n) q5.dequeue
      enqueue(n); n
    }
    def hasNext = true
    q2 enqueue 1; q3 enqueue 1; q5 enqueue 1
  }
}
\end{lstlisting-nobreak}

Then, just to prove the point, we used the ADR transformation to replace the mutable |Queue[Long]| by a very simple circular buffer implemented using an array and two indices. The transformation is clearly incorrect in the general case, as mutable containers should not be duplicated and, furthermore, in our very simplified implementation, we throw an exception if the buffer overflows. Yet, the many levels of optimization make the transformed program 4x faster.

We tested the programs in two scenarios: unsing the

\newcolumntype{Y}{>{\centering\arraybackslash}X}

\begin{table}
  \begin{tabularx}{\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark}        &  \textbf{Original}  & \textbf{Transformed} &      \textbf{Speedup} \\ \hline
    Hamming 2000, interpreter &                    42314 &                     8587 &                   4.9x \\
    Hamming 2000, with JIT    &                      687 &                      170 &                   4.0x \\
    Gaussian GCD, interpreter &                      127 &                       32 &                   3.9x \\
    Gaussian GCD, with JIT    &                       39 &                       22 &                   1.7x \\ \hline
  \end{tabularx}
  \caption{Running time for the benchmarks, in $\mu$seconds}
  \label{table:adrt}
  \vspace{-10mm}
\end{table}
% \begin{lstlisting-nobreak}
% class FunnyQueue {
%   private[this] val array = new Array[Long](20000)
%   private[this] var index_start = 0
%   private[this] var index_stop = 0
%   def enqueue(l: Long) = {
%     array(index_stop) = l
%     index_stop = (index_stop + 1) % 20000
%   }
%   def dequeue1(): Long = {
%     val res = array(index_start)
%     index_start = (index_start + 1) % 20000
%     res
%   }
%   def head1(): Long =
%     array(index_start)
% }
% \end{lstlisting-nobreak}

This is an outrageous transformation, since we do not protect against buffer overflows and we transform a mutable container. Yet, since the queue does not escape, it is a valid transformation that brings 5x speed improvements on average.

 for the general case, but is proves the point that, for our given transformation, it works perfectly.

\subsection{Retrofitting Specialization to Functions}
\label{sec:benchmarks:funcs}

\paragraph{Methodology: } In this work, we use 4 main microbenchmarks inspired
by~\cite{biboudis_clash_2014}. We focus our efforts on measuring iteration
throughput for a library that implements push-style streams in scala called
\emph{scala-streams}~\footnote{https://github.com/biboudis/scala-streams .}. In
all of our benchmarks we produce scalar values as the result of a terminal
operation. We measure the performance of:

\begin{itemize}
 \item \textbf{sum} iteration speed with no lambdas, just a single iteration.
 \item \textbf{sumOfSquares} a small pipeline with one map operation.
 \item \textbf{sumOfSquaresEven} a bigger pipeline with a filter and map chain..
 \item \textbf{cart} a nested pipeline with a \verb|flatMap| and an inner
   operation, again with a \verb|flatMap| (capturing a variable), to encode a
   Cartesian product.
\end{itemize}

For micro-benchmarks we use the Java Microbenchmark Harness
(JMH)~\cite{aleksey_shipilev_openjdk:_????} tool: a benchmarking tool for
JVM-based languages that is part of the OpenJDK.

\paragraph{Input:} All tests were run with the same input set. For the
\textbf{sum}, \textbf{sumOfSquares} and \textbf{sumOfSquaresEven} we used an
array of $N = 10,000,000$ long integers, produced by $N$ integers with a
\verb|Array.tabulate| function. The \textbf{cart} test iterates over two
arrays. An outer one of $1,000,000$ long integers and an inner one of $10$.

The scala-streams library introduces lazy streams as a potential alternative to
scala-views and its design is inspired by Nessos Streams~\cite{nessos_streams},
a library that implements push-style streams for F\# and C\#. Nessos Streams
implemented streams according to the Java 8 stream library design. In Java 8,
streams are fundamentaly continuation-based instead of iterator-based. The main
difference between the two methodologies is that although both are lazy, the
former is lazy for producers and the latter is lazy for consumers (who can
control the iteration through the methods of the iterator). A term often used to
describe such an internal iteration scheme is ``\emph{Push
  iteration}''~\cite{obsidian,defuncpush}.  The consumer of a Push stream will
provide a consumer function of type \verb|T => Boolean| that is instantiated
into the iteration block of the stream. The loop internally is controlled by the
return value of this function and also by the existance of more elements to be
processed. The dual of a Push stream is a~\emph{Pull stream}. Every combinator
of a Pull stream will build an iterator that will propagate some effect (e.g.,
apply a function \verb|f| if this combinator is \verb|map|) to each next
element. C\#, F\# and Scala implement deferred execution over pipelines as Pull
streams. The performance implications of each different design decision in
stream libraries was assessed in Biboudis et al.~\cite{biboudis_clash_2014}.

\begin{lstlisting}[language=Scala, caption=Example uses of scala-streams.]
var y = Stream(xs).filter(_*2==0).toArray()
var y = Stream(xs).map(_*2).fold(0)(_+_)
var y = Stream(lines).flatMap(line => Stream(line split "\\W+")).toArray
\end{lstlisting}

The implementation of scala-streams relies on deep nesting of loop-functions in
continuation-passing style. This makes scala-streams a very good candidate to
present the performance benefits of the automated data representation
transformation of function types for pipelines that perform number crunching.

\paragraph{Description of the library. }
A Push stream type has a single constructor. The \verb|Stream| constructor takes
as a parameter a loop-function which takes as a parameter another function with
type \verb|T => bool|.

In words, a stream holds a function that represents the entire iteration over
elements of an array, with another function applied to each one.  The
\verb|bool| return value serves to indicate whether the next consumer down the
operator chain should be called on the element or not.

The iteration itself is encoded as a loop (hereafter: the \verb|iterf|
function). The \verb|Stream| class constructs such a stream with the initial
looping function passed as the \verb|iterf| function. Note the correspondence
with the Java iteration scheme. Much like the Java method \verb|Stream.of|, the
scala-streams method apply yields a consumer that contains a predefined
iteration block (\verb|iterf|) and returns a new stream of that function. In
Listing~\ref{lst:operators} we present the construction of a stream and two
operators, an intermediate \verb|map| and a terminal \verb|foldLeft|. \verb|map|
takes as a parameter the mapping function and returns a new stream that has
updated the looping function (the consumer of the stream) by first applying
\verb|f| before passing the value to the next transformation function.
\aggelos{TODO2: I can produce figures but I use LibreOffice for my charts and
  all figures should be of the same style: my results are here:
  https://github.com/VladUreche/ildl-paper/blob/master/scala-streams-results.ods}

\begin{lstlisting-nobreak}[language=scala, caption=Creation of a stream \& examples of
  operators., label=lst:operators]
object Stream {
  @inline def apply[@miniboxed T: ClassTag](xs: Array[T]) = {
    val gen = (iterf: T => Boolean) => {
      var counter = 0
      var cont = true
      val size = xs.length
      while (counter < size && cont) {
        cont = iterf(xs(counter))
        counter += 1
      }
    }
  }
  new Stream(gen)
}
final class Stream[@miniboxed T: ClassTag](val streamf: (T => Boolean) => Unit) {
  def map[@miniboxed R: ClassTag](f: T => R): Stream[R] =
    new Stream(iterf => streamf(value => iterf(f(value))))

  def foldLeft[@miniboxed A](a: A)(op: (A, T) => A): A = {
    var acc = a
    streamf(value => {
      acc = op(acc, value)
      true
    })
    acc
  }
}
\end{lstlisting-nobreak}
\begin{figure}
  \centering
%  \includegraphics[width=.95\linewidth]{scala-streams.pdf}
  \caption{Microbenchmark of scala-streams. Y-axis in milliseconds / iteration (average of 10).}
  \label{fig:stream_benchmarks}
\end{figure}
\paragraph{Discussion: }  It is of high importance to provide an implementation
that works well under both reference types and primitives and ildl-plugin has
proven highly beneficial in scala-streams. Although scala-streams seem to
outperform Scala views, by using the \verb|@specialized| annotation we get
\aggelos{todo}x speedup and by using the ildl-plugin we get \aggelos{todo}x
speedup. In Figure~\ref{fig:stream_benchmarks}, we present the execution times
of our microbenchmarks. By running our experiments with the \verb|-perf perfasm|
flag of JMH, we are able to see that no internal boxing occurs. Instead, the
mapping function \verb|f| has been bridged with functions that use primitive
types as parameters and return types.

\aggelos{TODO3: Should we explain more the output of "-Xprint:minibox-commit"?: https://gist.github.com/biboudis/00b22abea986e496f060}

\aggelos{TODO4: Should we examine closely sumOfSquares JMH trace: no boxing occured!: https://gist.github.com/biboudis/1684edf24bb646450f3d}

