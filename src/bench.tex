\section{Benchmarks}
\label{sec:benchmarks}

%This section describes the benchmarks we ran on transformed programs,
%to evaluate whether the transformation is worthwhile.

We next evaluate the benefit of the ADR transformation experimentally.

\paragraph{Setup.} We executed all benchmarks on an Intel |i7-4702HQ| quad-core processor
machine with the frequency fixed at |2.2GHz|. The RAM available to the
benchmarks was |2GB|. The benchmarks were run either by executing the
benchmark 1000 times and measuring the total time, for the first
benchmark target, or by using the dedicated JMH benchmarking and
performance analysis platform \cite{aleksey_shipilev_openjdk:_????}
for the second benchmark target. The numbers presented are
preliminary, as we plan to re-execute all benchmarks using JMH on a
dedicated server machine for the possible camera-ready version of the paper.

We developed two Scala compiler plugins based on the ADR
transformation principles: the ADR plugin itself and an extension to
the miniboxing plugin \cite{miniboxing} that transforms the function
representation. We will present the benchmarks for each of these
transformations individually.

\subsection{Ad hoc Data Representation Transformations}
\label{sec:benchmarks:ad-hoc}

We tested the |adrt| Scala compiler plugin (\S\ref{sec:impl}) using two different benchmarks:

\begin{compactitem}
\item the Gaussian integer greatest common divisor algorithm, presented in \S\ref{sec:problem};
\item the algorithm for computing the first 2000 Hamming numbers.
\end{compactitem}

For the Gaussian integer benchmark, we replaced the pair-based Gaussian integer representation by a long integer. Furthermore, we encoded all the operations, such as |+|, |-|, |/|, |%| and others as extension methods:

\begin{lstlisting-nobreak}
adrt(IntPairComplexToLongComplex) {
  def gcd(n1: (Int, Int), n2: (Int, Int)): (Int, Int) = {
    val remainder = n1 % n2
    if (remainder.norm == 0) n2 else gcd(n2, remainder)
  }
}
\end{lstlisting-nobreak}

For the Hamming numbers benchmark, we took the algorithm from the Rosetta Code website\footnote{Available at: \url{http://rosettacode.org/wiki/Hamming_numbers#Scala}.} and manually replaced all references to |BigInt| by |Long|:

\begin{lstlisting-nobreak}
adrt(QueueOfIntAsFunnyQueue) {
  class Hamming extends Iterator[Long] {
    import scala.collection.mutable.Queue
    val q2 = new Queue[Long]
    val q3 = new Queue[Long]
    val q5 = new Queue[Long]
    def enqueue(n: Long) = {
      q2 enqueue n * 2
      q3 enqueue n * 3
      q5 enqueue n * 5
    }
    def next = {
      val n = q2.head min q3.head min q5.head
      if (q2.head == n) q2.dequeue
      if (q3.head == n) q3.dequeue
      if (q5.head == n) q5.dequeue
      enqueue(n); n
    }
    def hasNext = true
    q2 enqueue 1
    q3 enqueue 1
    q5 enqueue 1
  }
}
\end{lstlisting-nobreak}

Then, just to prove the point, we used the ADR transformation to replace the mutable |Queue[Long]| by a very simple circular buffer implemented using an array and two indices. The transformation is clearly incorrect in the general case, since converting mutable containers breaks aliasing and, furthermore, since in our very naive implementation, we throw an exception if the buffer overflows. Yet, the aggressive assumptions about the code, backed up by exceptions that prevent incorrect behavior from passing unnoticed allow us to improve the performance by a factor of 4x.

\newcolumntype{Y}{>{\centering\arraybackslash}X}

\begin{table}[t!]
  \begin{tabularx}{\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark}        &  \textbf{Original}  & \textbf{Transformed} &      \textbf{Speedup} \\ \hline
    Hamming 2000, interpreter &                    42314 &                     8587 &                   4.9x \\
    Hamming 2000, with JIT    &                      687 &                      170 &                   4.0x \\
    Gaussian GCD, interpreter &                      127 &                       32 &                   3.9x \\
    Gaussian GCD, with JIT    &                       39 &                       22 &                   1.7x \\ \hline
  \end{tabularx}
  \vspace{1mm}
  \caption{Running time for the transformation benchmarks, in $\mu$seconds.}
  \label{table:adrt}
  \vspace{-9mm}
\end{table}
% \begin{lstlisting-nobreak}
% class FunnyQueue {
%   private[this] val array = new Array[Long](20000)
%   private[this] var index_start = 0
%   private[this] var index_stop = 0
%   def enqueue(l: Long) = {
%     array(index_stop) = l
%     index_stop = (index_stop + 1) % 20000
%   }
%   def dequeue1(): Long = {
%     val res = array(index_start)
%     index_start = (index_start + 1) % 20000
%     res
%   }
%   def head1(): Long =
%     array(index_start)
% }
% \end{lstlisting-nobreak}

The results in Table \ref{table:adrt} suggest the ADR transformation can speed up container-based code by factors between 1.7 and 4.9x.

\subsection{Retrofitting Specialization to Functions}
\label{sec:benchmarks:funcs}

% subset of the adrt, missing frontend features -- value annotation and signature persistance are there.
% motivation: miniboxing specialization + function encoding
A second Scala compiler plugin we implemented formed the basis of the ADR transformation idea. Although it fixes the transformation, it is scope-based, it persists annotations and it uses bridges to maintain the overriding status. The motivation for this plugin is transforming the anonymous functions emitted by the Scala compiler in order to optimize their invocation from miniboxing-specialized code. We will briefly explain miniboxing and the Scala functions.

% miniboxing transformation - show def apply(...) = ...
The miniboxing transformation \cite{miniboxing} proposes an alternative to erasure, allowing generic methods and classes to work efficiently with unboxed primitive types. Unlike the current specialization transformation in the Scala compiler \cite{iuli-thesis}, which duplicates and adapts the generic code once for every primitive type, the miniboxing transformation only duplicates the code once, and encodes all primitive types inside the long integer. This allows it to scale much better than specialization \cite{miniboxing-linkedlist} in terms of bytecode size while providing comparable performance. Yet, one of the main drawbacks of using the miniboxing plugin is that all Scala library classes are either generic or specialized with the early specialization scheme, which is not compatible to miniboxing. For example, interacting with functions and tuples incurs significant overhead.

% object oriented function representation
The Scala programming language offers functions as first-class citizens. However, since functions are not first-class citizens in the Java Virtual Machine bytecode, the compiler has to transform them to anonymous classes extending a functional interface. The following example shows the desugaring of function |(x: Int) => x + 1|:

\begin{lstlisting-nobreak}
class $anon extends Function1[Int, Int] {
  def apply(x: Int): Int = x + 1
}
new $anon()
\end{lstlisting-nobreak}

% not transformed according to miniboxing. What we'd like to have is ...
The |Function1| trait is specialized using the early scheme in the Scala compiler. This means that the function will expose more versions of the |apply| method, including one which accepts an unboxed integer and returns an unboxed integer. However, it will not expose a version that encodes the primitive type as a long integer, as the miniboxing transformation expects. Therefore, when programmers write miniboxed code that uses functions, they have two choices: either accept the slowdown of boxing or define their own |MiniboxedFunction1| class, and perform the desugaring by hand. Neither of these is a good solution.

% retrofitting the function representation - after the fact, as parts of the
What we would like to have is a way to transform the references to |Fucntion1| in miniboxed code: instead of extending |Function1|, anonymous functions should extend |MiniboxedFunction1|. But the problem is the miniboxed code needs to interoperate with library-defined code out there, or with other libraries that were not transformed. This is where the scope comes in: the miniboxed code acts as a scope for the function representation transformation!

% the benchmark: scala streams, based on ...
\subsubsection{The benchmark} we used for the function representation transformation is based on a stream library\footnote{Available at \url{https://github.com/biboudis/scala-streams} .} described in \cite{biboudis_clash_2014}. The main design choice is to introduce lazy, continuation-based stream transformations, inspired by Nessos Streams \cite{nessos_streams} and the newly added Java 8 streams. Being continuation-based, each stream combinator provides a function that is registered in the stream. These functions are stacked together to form a transformation pipeline. When the consumer wants to read a value, it starts from the original stream source, which is usually an array or another collection, and executes all the stacked functions one after another. For example:

\begin{lstlisting-nobreak}
val x = Stream(numbers).map(_*2).fold(0)(_+_)
val y = Stream(lines).flatMap(line => Stream(line split "\\W+")).toArray
\end{lstlisting-nobreak}

On the implementation side, the |Stream| class has a single constructor, which takes a loop-function as parameter. Instead of the expected type |T => Unit|, the type of the loop function is |(T => Boolean) => Unit|. This allows it to indicate whether the next consumer should see the value or not, in turn enabling combinators such as |filter|, |take| and |takeWhile|:

\begin{lstlisting-nobreak}
final class Stream[@miniboxed T](val streamf: (T => Boolean) => Unit) {

  def map[@miniboxed R: ClassTag](f: T => R): Stream[R] =
    new Stream(iterf => streamf(value => iterf(f(value))))

  def filter(p: T => Boolean): Stream[T] =
    new Stream(iterf => streamf(value => !p(value) || iterf(value)))

  def takeWhile(p: T => Boolean): Stream[T] =
    new Stream(iterf => streamf(value => if (p(value)) iterf(value) else false))
}
\end{lstlisting-nobreak}

% measure 5 different benchmarks ...
The implementation of scala-streams relies on deep nesting of loop-functions in continuation-passing style, making it a very good candidate to benchmark the function representation transformation. To this end, we measure four operations:

\begin{compactitem}
 \item \textsc{Sum} -- single iteration with no lambdas;
 \item \textsc{SumOfSquares} -- a small pipeline with a |map| operation;
 \item \textsc{SumOfSquaresEven} -- a bigger pipeline with a |filter| and |map| chain;
 \item \textsc{Cartesian} -- a nested pipeline using |flatMap| to encode Cartesian products.
\end{compactitem}

The input for the \textsc{sum}, \textsc{sumOfSquares} and \textsc{sumOfSquaresEven} benchmarks is an array of 10 million long integers. The \textsc{cart} benchmark iterates over two arrays: An outer one of 10M long integers and an inner one of just 10.

\begin{table}[t]
  \begin{tabularx}{\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
                              &                    & \textbf{Miniboxing}& \textbf{Miniboxing} \\
    \rowcolor{Gray}
    \textbf{Benchmark}        &  \textbf{Generic}  &  without function support & with function support \\ \hline
    \textsc{Sum}              &              100.6 &              355.9 &             12.0 \\
    \textsc{SumOfSquares}     &              188.3 &              450.9 &             13.0 \\
    \textsc{SumOfSquaresEven} &              130.8 &              300.4 &             52.2 \\
    \textsc{Cart}             &              220.6 &              560.2 &             55.3 \\ \hline
  \end{tabularx}
  \vspace{1mm}
  \caption{Running time for the streams benchmarks, in milliseconds.}
  \label{table:streams}
  \vspace{-9mm}
\end{table}

% results ...
The results in Table \ref{table:streams} show the impact of the function transformation, which is at the heart of the streams library performance. We can see that using the miniboxing transformation for the |Stream| class without the function transformation degrades the performance by factors between 2.3 and 3.5x. Yet, when the miniboxing transformation is complemented by our function representation transformation, the performance increases compared to the generic case by factors between between 2.5x and 14.5x.


