\section{Benchmarks}
\label{sec:benchmarks}
\label{sec:benchmarks:ad-hoc}

This section evaluates the experimental benefits of ADR transformations in targeted micro-benchmarks and in
the setting of a library and its clients.
% and shows four different usage scenarios.
%
%\subsection{Setup}
%
We ran the benchmarks on an Intel |i7-4702HQ| quad-core processor machine with the frequency fixed at |2.2GHz|. The RAM available to the benchmarks was |2GB| and the running times were measured using the scalameter benchmarking platform \cite{scalameter}, to avoid jitter from the garbage collector and just-in-time compiler.


\subsection{ADRT Micro-Benchmarks}

We chose representative micro-benchmarks in order to cover a wide range of transformations using the |adrt| scope:

\begin{itemize}
\item the greatest common divisor algorithm, presented in \S\ref{sec:problem};
\item least squares benchmark + deforestation \cite{wadler-deforestation};
\item averaging sensor readings + array of struct;
\item computing the first 10000 Hamming numbers.
\end{itemize}

\noindent
The benchmarks were optimized using our implementation of the |adrt| scope at \cite{ildl-plugin} and are described in detail on the website \cite{ildl-plugin-wiki}. We will proceed to explain the transformation in each benchmark, but, due to space constraints, the full descriptions are only available on the website.

\subsubsection{The Gaussian Greatest Common Divisor}
% What is the benchmark about?
% Why does it matter?
% How we optimized it?
is the running example described in \S\ref{sec:problem} and used throughout the paper. It is a numeric and CPU-bound benchmark, where the main slowdown is caused by heap allocations.

The |adrt| transformation surrounds the tail-recursive |gcd| method and optimizes the pair of integer representation of Gaussian integers by encoding them in long integers. From this point of view, we can think of the transformation as retrofitting the value class status to the pair of integers. The benchmark results in Table \ref{table:adrt} show a 13x speed improvement and we checked that no object allocations occur in the transformed version of the |gcd| method. The transformation description object is 30 lines of code (LOC) long.

\subsubsection{The Least Squares Method} takes a list of points in two dimensions and computes the slope and offset of a straight line that best approximates the input data. The benchmark performs multiple traversals over the input data and thus benefits from deforestation \cite{wadler-deforestation}, which avoids the creation of intermediate collections after each |map| operation:

\begin{lstlisting-nobreak}
adrt(ListAsLazyList){
  def leastSquares(data: List[(Double, Double)]) = {
    val size = data.length
    val sumx = data.map(_._1).sum
    val sumy = data.map(_._2).sum
    val sumxy = data.map(p => p._1 * p._2).sum
    val sumxx = data.map(p => p._1 * p._1).sum
    ...
  }
}
\end{lstlisting-nobreak}

\noindent
The |adrt| scope performs a generic transformation from |List[T]| to |LazyList[T]|:

\begin{lstlisting-nobreak}
object ListAsLazyList extends TransformationDescription {
  def toRepr[T](list: List[T]): LazyList[T] = ...
  def toHigh[T](list: LazyList[T]): List[T] = ...
  // bypass methods
}
\end{lstlisting-nobreak}

The |LazyList| collection achieves deforestation by recording the mapped functions and executing them lazily, either when |force| is invoked on the collection or when a |fold| operation is executed. Since the |sum| operation is implemented as a |foldLeft|, the |LazyList| applies the function and sums the result without creating an intermediate collection. This transformation alone produced a 3x speedup for an input of 5 million points and made the |adrt| transformation perform better than the dedicated collection optimization tool |scalablitz| \cite{scalablitz, scalablitz-paper}, which produced a modest 1.7x speedup.

Yet, using the |LazyList| collection we can also benefit from specialization \cite{iuli-thesis}. Using a specialized version of the |LazyList| collection we obtain a 5x speed improvement thanks to a combination of deforestation and specialization. Therefore, we used the |adrt| transformation both to transform the collection semantics and to retrofit specialization in a localized scope. The transformation description object is 30 LOC and the |LazyList| is 70 LOC.

Although the Least Squares Method benchmark is a specialized micro-benchmark, the techniques it uses are of wide applicability. For instance, deforestation can be applied in a variety of realistic optimization scenarios.



\subsubsection{The Sensor Readings} benchmark was inspired by the Sparkle visualization tool \cite{sparkle}, which is able to quickly display, zoom, transform and filter sensor readings. To obtain nearly real-time results, Sparkle combines several optimizations such as streaming and array-of-struct to struct-of-array conversions, all currently implemented by hand. In our benchmark, we implemented a mock-up of the processing core of Sparkle and automated the array-of-struct transform:

\begin{lstlisting-nobreak}
type SensorReadings = Array[(Long, Long, Double)]
class StructOfArray(arrayOfTimestamps: Array[Long],
                           arrayOfEvents:     Array[Long],
                           arrayOfReadings:   Array[Double])

object AoSToSoA extends TransformationDescription {
  def toRepr(aos: SensorReadings): StructOfArray = ...
  def toHigh(soa: StructOfArray): SensorReadings = ...
  ...
}
\end{lstlisting-nobreak}

Using the |adrt| scope produced a
%modest
speedup of 2.2x.
%, which initially puzzled us, since we were expecting better speedups. When
For comparison, we implemented the transformation by hand, manually refactoring the code, which obtained a nearly-identical speedup of 2.25x.
%, showing this is the best that can be achieved for the transformation. Still, this shows the |adrt| scope is able to change the underlying data structures and provide speedups while maintaining program semantics.
The transformation description object is 60 LOC and the optimization applied (array-of-struct to struct-of-array) is general-purpose.


\subsubsection{The Hamming Numbers Benchmark} computes numbers that only have 2, 3 and 5 as their prime factors, in order. Unlike the other benchmarks, this is an example we randomly picked from Rosetta Code \cite{rosetta-code} and attempted to speed up:

\begin{lstlisting-nobreak}
adrt(BigIntToLong) {
  adrt(QueueOfBigIntAsFunnyQueue) {
    class Hamming extends Iterator[BigInt] {
      import scala.collection.mutable.Queue
      val q2 = new Queue[BigInt]
      val q3 = new Queue[BigInt]
      val q5 = new Queue[BigInt]
      def enqueue(n: BigInt) = {
        q2 enqueue n * 2
        q3 enqueue n * 3
        q5 enqueue n * 5
      }
      def next = {
        val n = q2.head min q3.head min q5.head
        if (q2.head == n) q2.dequeue
        if (q3.head == n) q3.dequeue
        if (q5.head == n) q5.dequeue
        enqueue(n); n
      }
      def hasNext = true
      q2 enqueue 1
      q3 enqueue 1
      q5 enqueue 1
    }
  }
}
\end{lstlisting-nobreak}

An observation is that, for the first 10000 Hamming numbers, there is no need to use |BigInt|, since the numbers fit into a |Long| integer. Therefore, we used two nested |adrt| scopes to replace |BigInt| by |Long| and |Queue[BigIng]| by a fixed-size circular buffer built on an array. The result was an 8x speedup. The main point in the transformation is its optimistic nature, which makes the assumption that, for the Hamming numbers we plan to extract, the long integer and a fixed-size circular buffer are good enough. This is similar to what a dynamic language virtual machine would do: it would make assumptions based on the code and would automatically de-specialize the code if the assumption is invalidated. In our case, when the assumption is invalidated, the code will throw an exception. For this example, the transformation has 100 LOC.


\newcolumntype{Y}{>{\centering\arraybackslash}X}

\begin{table}[t!]
  \centering
  \begin{tabularx}{0.48\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark}   &  \textbf{Original}  &     \textbf{ADRT} &      \textbf{Speedup} \\ \hline
    Gaussian GCD         &        3.05 $\mu$s &      0.23 $\mu$s &                   13x \\
    Least Sq. Blitz (5M) &             8026 ms &           4763 ms &                  1.7x \\
    Least Sq. adrt 1 (5M) &             8026 ms &           2393 ms &                    3x \\
    Least Sq. adrt 2 (5M) &             8026 ms &           1643 ms &                    5x \\
    Sensor Readings (5M) &             50.8 ms &           23.1 ms &                    2x \\
    Hamming 10000th      &             4.35 ms &           0.55 ms &                    8x \\ \hline
  \end{tabularx}
  \vspace{-2mm}
  \caption{Benchmark running time for each use case.}
  \label{table:adrt}
  \vspace{-1em }
\end{table}

\subsection{ADRT in Realistic Libraries}
% \subsection{Retrofitting Specialization to Functions}
\label{sec:benchmarks:funcs}

% subset of the adrt, missing frontend features -- value annotation and signature persistence are there.
% motivation: miniboxing specialization + function encoding

The |adrt| scoped transformation is a conceptual generalization of a mechanism
motivated by library transformation scenarios. In particular, the
resulting data representation transformation is used in conjunction
with the miniboxing transformation \cite{miniboxing-www, miniboxing},
in order to replace standard library \emph{functions} and \emph{tuples}
by custom, optimized versions adequate for miniboxed code.
The scope of this data representation transformation is miniboxing-transformed code.

%Before the |adrt| scope was developed, we worked on a plugin that used a similar scoped approach: an extension used by the miniboxing transformation \cite{miniboxing-www, miniboxing} to replace library functions and tuples by custom optimized versions. Although this plugin was a precursor of the |adrt| transformation, being scoped but not programmer-driven, we present it because in the meantime it became part of the miniboxing plugin and has been used to transform large pieces of code, proving our approach scales not only to toy examples, but to actual code used in the industry.

% miniboxing transformation - show def apply(...) = ...
The miniboxing transformation \cite{miniboxing} proposes an alternative to erasure, allowing generic methods and classes to work efficiently with unboxed primitive types. Unlike the current specialization transformation in the Scala compiler \cite{iuli-thesis}, which duplicates and adapts the generic code once for every primitive type, the miniboxing transformation only duplicates the code once and \emph{encodes all primitive types in long integers}. This allows miniboxing to scale much better than specialization \cite{miniboxing-linkedlist} in terms of bytecode size while providing comparable performance. Yet, one of the main drawbacks of using the miniboxing plugin is that all Scala library classes are either generic or specialized with the built-in Scala specialization scheme, which is not compatible with miniboxing. Therefore, interacting with functions and tuples from miniboxed code incurs significant overhead.

% object oriented function representation
Consider, for example, functions. (Tuples raise similar issues.) Scala offers functions as first-class citizens. However, since functions are not first-class citizens in the Java Virtual Machine bytecode, the Scala compiler desugars them to anonymous classes extending a functional interface. The following example shows the desugaring of function |(x: Int) => x + 1|:

\begin{lstlisting-nobreak}
class $anon extends Function1[Int, Int] {
  def apply(x: Int): Int = x + 1
}
new $anon()
\end{lstlisting-nobreak}

This function desugaring does not expose a version of the |apply| method that encodes the primitive type as a long integer, as the miniboxing transformation expects. Therefore, when programmers write miniboxed code that uses functions, they have two choices: either accept the slowdown caused by converting the representation or define their own miniboxed |Function1| class, and perform the function desugaring by hand. Neither of these is a good solution.

% retrofitting the function representation - after the fact, as parts of the
%What we would like to have is a way to transform the references to |Function1| in miniboxed code: instead of extending |Function1|, anonymous functions should extend |MiniboxedFunction1|. But the problem is that the miniboxed code needs to interoperate with library-defined code, or with other libraries that were not transformed. This is where the scope comes in: the miniboxed code acts as a scope for the \emph{function and tuple representation transformation}, i.e., the ADR transformation of |Function| and |Tuple|.

Our data representation transformation converts the references to
|Function1| in miniboxed code to the optimized |MiniboxedFunction1|,
which allows calls to use the miniboxed representation, thus being more efficient. The
problem is that the miniboxed code needs to interoperate with
library-defined code, or with other libraries that were not
transformed. Thus the miniboxed code acts
as a scope for the \emph{function and tuple representation
  transformation}, i.e., the ADR transformation of |Function| and
|Tuple|.
%We will now focus on two published benchmarks that exercise the function and tuple optimization. % done using our |adrt| precursor transformation.
This transformation has a significant impact in library benchmarks.

\begin{table}[t]
  \begin{tabularx}{0.48\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark} & \textbf{Generic} & \textbf{Miniboxed}& \textbf{Miniboxed} \\
    \rowcolor{Gray}
                       &                  &                   & +functions \\ \hline
    Sum              &              100.6 ms &              355.9 ms &             12.0 ms \\
    SumOfSquares     &              188.3 ms &              450.9 ms &             13.0 ms \\
    SumOfSqEven      &              130.8 ms &              300.4 ms &             52.2 ms \\
    Cart             &              220.6 ms &              560.2 ms &             55.3 ms \\ \hline
  \end{tabularx}
  \vspace{-2mm}
  \caption{Scala Streams pipelines for 10M elements.}
  \label{table:streams}
  \vspace{-1em}
\end{table}


\subsubsection{The Scala-Streams library} \cite{biboudis_clash_2014} imitates the design of the Java 8 stream library, to achieve high performance (relative to standard Scala libraries) for functional operations on data streams. The library is available as an open-source implementation \cite{biboudis-streams}. In its continuation-based design, each stream combinator provides a function that is stacked to form a transformation pipeline. As the consumer reads from the final stream, the transformation pipeline is executed, processing an element from the source into an output element. However, the pipeline architecture is complex, since combinators such as |filter| may drop elements, stalling the pipeline.
% This makes the Scala Streams an interesting platform to study the performance benefits of the miniboxing transformation and, in turn, of our |adrt| precursor.

Table \ref{table:streams} shows the result of applying our data
representation transformation to the Scala-Streams published
benchmarks. (The benchmarks are described in detail in prior
literature \cite{biboudis_clash_2014}.) As can be seen, the miniboxing
transformation is an enabler of our optimization but produces
\emph{worse} results by itself (due to extra conversions).

Compared to the original library, the application of miniboxing and
data representation optimization for functions achieves a very high
speedup---up to 14.5x for the SumOfSquares benchmark. In fact, the
speedup relative to the miniboxed code without the function
representation optimization is nearly 35x!

%Without going into the details of the benchmarks, which are covered in \cite{biboudis_clash_2014}, Table \ref{table:streams} shows the results with and without our |adrt| precursor extension, showing up to 14.5x speedups when functions are optimized.


\subsubsection{The Framian Vector implementation} is an exploration into deeply specializing the immutable |Vector| bulk storage without using reified types \cite{tixxit-respecialization15,tixxit-respecialization6}. This is a benchmark created by a commercial entity using the Scala programming language. Table \ref{table:framian} shows a 4.4x speed improvement when the function representation is optimized and shows the ADR-transformed function code lies within a 10\% margin compared to the fully specialized and manually optimized code.

% normal exec:
% [info] Benchmark                                       Mode  Samples        Score  Score error  Units
% [info] r.VecMapBenchmark.squareDoubleArrayWithLoop    thrpt       20  1532951.740    10201.172  ops/s
% [info] r.VecMapBenchmark.squareDoubleArrayWithMap     thrpt       20    74578.832      644.821  ops/s
% [info] r.VecMapBenchmark.squareDoubleVec              thrpt       20  1418579.315    10376.822  ops/s
% with -P:minibox:library-functions
% [info] Benchmark                                       Mode  Samples        Score  Score error  Units
% [info] r.VecMapBenchmark.squareDoubleArrayWithLoop    thrpt       20  1540313.539    18644.313  ops/s
% [info] r.VecMapBenchmark.squareDoubleArrayWithMap     thrpt       20    89519.549      760.324  ops/s
% [info] r.VecMapBenchmark.squareDoubleVec              thrpt       20   324700.925     3609.481  ops/s
\begin{table}[t]
  \begin{tabularx}{0.48\textwidth}{|g *{1}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark}             &  \textbf{Running time} \\ \hline
    Manual C-like code             &         0.650 $\mu$s \\
    Miniboxing with functions      &         0.705 $\mu$s \\
    Miniboxing without functions   &         3.080 $\mu$s \\
    Generic                        &        13.409 $\mu$s \\ \hline
  \end{tabularx}
  \vspace{-2mm}
  \caption{Mapping a 1K vector.}
  \label{table:framian}
  \vspace{-1em}
\end{table}

%We will close the section by concluding that the |adrt| scopes are capable of covering a broad range of custom and scoped transformations and the technique has been shown to scale to large programs, through the miniboxing plugin extension based on the |adrt| approach.
