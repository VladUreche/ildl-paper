\section{Benchmarks}
\label{sec:benchmarks}

This section describes the benchmarks we ran on transformed programs, to evaluate whether the transformation is worthwhile.

We executed all benchmarks on a laptop computer with an Intel |i7-4702HQ| quad-core processor with the frequency fixed at |2.2GHz|. The RAM memory available to the benchmarks was |2GB| of RAM. The benchmarks were ran either by executing the benchmark 1000 times and measuring the total time, for the first benchmark target, or by using the dedicated JHM benchmarking and performance analysis platform \cite{aleksey_shipilev_openjdk:_????} for the second benchmark target. The numbers presented are preliminary, as we plan to re-execute all benchmarks using JMH on a dedicated server machine for the camera-ready version of the paper.

We developed two Scala compiler plugins based on the ADR transformation principles: the ADR plugin itself and an extension to the miniboxing plugin \cite{miniboxing} that transforms the function representation. We will present the benchmarks for each of these transformations individually.

\subsection{Ad hoc Data Representation Transformations}
\label{sec:benchmarks:ad-hoc}

We tested the |adrt| Scala compiler plugin (\S\ref{sec:impl}) using two different benchmarks:

\begin{compactitem}
\item the Gaussian integer greatest common divisor algorithm, presented in \S\ref{sec:problem};
\item the algorithm for computing the first 2000 Hamming numbers.
\end{compactitem}

For the Gaussian integer benchmark, we replaced the pair-based Gaussian integer representation by a long integer. Furthermore, we encoded all the operations, such as |+|, |-|, |/|, |%| and others as extension methods:

\begin{lstlisting-nobreak}
adrt(IntPairComplexToLongComplex) {
  def gcd(n1: (Int, Int), n2: (Int, Int)): (Int, Int) = {
    val remainder = n1 % n2
    if (remainder.norm == 0) n2 else gcd(n2, remainder)
  }
}
\end{lstlisting-nobreak}

For the Hamming numbers benchmark, we took the algorithm from the Rosetta Code website\footnote{Available at: \url{http://rosettacode.org/wiki/Hamming_numbers#Scala}.} and manually replaced all references to |BigInt| by |Long|:

\begin{lstlisting-nobreak}
adrt(QueueOfIntAsFunnyQueue) {
  class Hamming extends Iterator[Long] {
    import scala.collection.mutable.Queue
    val q2 = new Queue[Long]; val q3 = new Queue[Long]; val q5 = new Queue[Long]
    def enqueue(n: Long) = {
      q2 enqueue n * 2; q3 enqueue n * 3; q5 enqueue n * 5
    }
    def next = {
      val n = q2.head min q3.head min q5.head
      if (q2.head == n) q2.dequeue; if (q3.head == n) q3.dequeue; if (q5.head == n) q5.dequeue
      enqueue(n); n
    }
    def hasNext = true
    q2 enqueue 1; q3 enqueue 1; q5 enqueue 1
  }
}
\end{lstlisting-nobreak}

Then, just to prove the point, we used the ADR transformation to replace the mutable |Queue[Long]| by a very simple circular buffer implemented using an array and two indices. The transformation is clearly incorrect in the general case, since converting mutable containers breaks aliasing and, furthermore, since in our very naive implementation, we throw an exception if the buffer overflows. Yet, the aggressive assumptions about the code, backed up by exceptions that prevent incorrect behavior from passing unnoticed allow us to improve the performance by a factor of 4x.

\newcolumntype{Y}{>{\centering\arraybackslash}X}

\begin{table}
  \begin{tabularx}{\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark}        &  \textbf{Original}  & \textbf{Transformed} &      \textbf{Speedup} \\ \hline
    Hamming 2000, interpreter &                    42314 &                     8587 &                   4.9x \\
    Hamming 2000, with JIT    &                      687 &                      170 &                   4.0x \\
    Gaussian GCD, interpreter &                      127 &                       32 &                   3.9x \\
    Gaussian GCD, with JIT    &                       39 &                       22 &                   1.7x \\ \hline
  \end{tabularx}
  \caption{Running time for the transformation benchmarks, in $\mu$seconds.}
  \label{table:adrt}
  \vspace{-10mm}
\end{table}
% \begin{lstlisting-nobreak}
% class FunnyQueue {
%   private[this] val array = new Array[Long](20000)
%   private[this] var index_start = 0
%   private[this] var index_stop = 0
%   def enqueue(l: Long) = {
%     array(index_stop) = l
%     index_stop = (index_stop + 1) % 20000
%   }
%   def dequeue1(): Long = {
%     val res = array(index_start)
%     index_start = (index_start + 1) % 20000
%     res
%   }
%   def head1(): Long =
%     array(index_start)
% }
% \end{lstlisting-nobreak}

The results in Table \ref{table:adrt} show the ADR transformation is both useful in practice and can speed up programs significantly.

\subsection{Retrofitting Specialization to Functions}
\label{sec:benchmarks:funcs}

% subset of the adrt, missing frontend features -- value annotation and signature persistance are there.
% motivation: miniboxing specialization + function encoding
A second Scala compiler plugin we implemented formed the basis of the ADR transformation idea. Although it hardcodes the transformation, it is scope-based, it persists annotations in the signatures and introduces bridge methods to maintain the object model. The motivation for this plugin is transforming the anonymous functions emitted by the Scala compiler in order to optimize their invocation from miniboxing-specialized code. We will briefly explain miniboxing and the Scala anonymous functions.

% miniboxing transformation - show def apply(...) = ...
The miniboxing transformation \cite{miniboxing} proposes an alternative to erasure, allowing generic methods and classes to be used with unboxed primitive types. But, unlike the current specialization transformation in Scala \cite{iuli-thesis}, which duplicates and adapts the generic code for every primitive type, the miniboxing transformation only duplicates the code once, and uses a long integer to encode all primitive types. This allows it to scale much better than specialization \cite{miniboxing-linkedlist} while providing comparable performance. Yet, one of the main drawbacks of using the miniboxing plugin is that all Scala library classes are either generic or specialized with the early specialization scheme, which is not compatible to miniboxing.

% object oriented function representation
The Scala compiler offers first-class functions in the language. However, since functions are not first-class citizens in the Java Virtual Machine bytecode, the compiler has to transform them to anonymous classes extending a functional interface. The following example shows the desugaring for function $x => x + 1$:

\begin{lstlisting-nobreak}
class $anon extends Function1[Int, Int] {
  def apply(x: Int): Int = x + 1
}
new $anon()
\end{lstlisting-nobreak}

% not transformed according to miniboxing. What we'd like to have is ...
The |Function1| trait is specialized using the existing transformation in the Scala compiler. This means that the function will expose more versions of the |apply| method, including one which accepts an unboxed integer and returns an unboxed integer. However, it will not expose a version that encodes the primitive type as a long integer, as the miniboxing transformation expects. Therefore, when programmers write miniboxed code that uses functions, they have two choices: either accept the slowdown of boxing or define thir own |MiniboxedFunction1| class, marked for miniboxing transformation, and perform all desugaring by hand. Neither is a good solution.

% retrofitting the function representation - after the fact, as parts of the
What we would like to have is a way to transform the references to |Fucntion1| in miniboxed code: instead of extending |Function1|, anonymous functions should extend |MiniboxedFunction1|. Instead of having to box to call a method, the miniboxed code should be able to invoke it directly. Modulo the hardcoded semantics, the transformation is an ah-hoc, scope-based representation transformation. And, indeed, this was the first transformation we implemented, before working on the programmer-driven ADR transformation plugin.

% the benchmark: scala streams, based on ...
\subsubsection{The benchmark} we used for the function representation transformation is based on a stream library\footnote{Available at \url{https://github.com/biboudis/scala-streams}.} described in \cite{biboudis_clash_2014}. The main design choice is to introduce lazy, continuation-based stream transformations, inspired by Nessos Streams \cite{nessos_streams} and the newly added Java 8 streams. Being continuation-based, each stream combinator provides a function that is registered in the stream. These functions are stacked together to form a transformation pipeline. When the consumer wants to read a value, it starts from the original stream source, which is usually an array or another collection, and executes all the stacked functions one after another. For example:

\begin{lstlisting-nobreak}
val x = Stream(numbers).map(_*2).fold(0)(_+_)
val y = Stream(lines).flatMap(line => Stream(line split "\\W+")).toArray
\end{lstlisting-nobreak}

On the implementation side, the |Stream| class has a single constructor, which takes a loop-function as parameter. Instead of the expected type |T => Unit|, the type of the loop function is |(T => Boolean) => Unit|. This allows it to indicate whether the next consumer should see the value or not, in turn enabling combinators such as |filter|, |take| and |takeWhile|:

\begin{lstlisting-nobreak}
final class Stream[@miniboxed T: ClassTag](val streamf: (T => Boolean) => Unit) {
  def map[@miniboxed R: ClassTag](f: T => R): Stream[R] =
    new Stream(iterf => streamf(value => iterf(f(value))))

  def foldLeft[@miniboxed A](a: A)(op: (A, T) => A): A = {
    var acc = a
    streamf(value => {
      acc = op(acc, value)
      true
    })
    acc
  }
}
\end{lstlisting-nobreak}

% measure 5 different benchmarks ...
The implementation of scala-streams relies on deep nesting of loop-functions in continuation-passing style. This makes it a very good candidate to benchmark the benefits of transforming the function representation. To this end, we measure four operations:

\begin{compactitem}
 \item sum -- single iteration with no lambdas;
 \item sumOfSquares -- a small pipeline with one map operation;
 \item sumOfSquaresEven -- a bigger pipeline with a filter and map chain;
 \item cart -- a complex nested pipeline using |flatMap| to encode Cartesian product.
\end{compactitem}

The input for the \textem{sum}, \textem{sumOfSquares} and \textem{sumOfSquaresEven} benchmarks is an array of $N = 10,000,000$ long integers. The \textem{cart} benchmark iterates over two arrays: An outer one of $1,000,000$ long integers and an inner one of $10$.

\begin{table}[t]
  \begin{tabularx}{\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark}        &  \textbf{Generic}  & \textbf{Miniboxed} & \textbf{Miniboxed} \\
    \rowcolor{Gray}
                              &                    & without function support& with function support \\ \hline
    sum              &              100.6 &              355.9 &             12.0 \\
    sumOfSquares     &              188.3 &              450.9 &             13.0 \\
    sumOfSquaresEven &              130.8 &              300.4 &             52.2 \\
    cart             &              220.6 &              560.2 &             55.3 \\ \hline
  \end{tabularx}
  \caption{Running time for the streams benchmarks, in milliseconds.}
  \label{table:streams}
  \vspace{-10mm}
\end{table}

% results ...
The results in Table \ref{table:streams} show the impact of the function transformation, which is at the heart of the streams library performance. We can see that using the miniboxing transformation for the |Stream| class without the function transformation degrades the performance by factors between 2.3 and 3.5x. Yet, when the miniboxing transformation is complemented by our function representation transformation, the performance increases compared to the generic case by factors between between 2.5x and 14.5x. This improves our confidence that


