\section{Introduction}
\label{sec:intro}

% Objects = encapsulated data + API. Extension methods = ad hoc additions of the API
An object encapsulates code and data and exposes an interface. Modern
language facilities, such as extension methods, type classes and
implicit conversions allow programmers to evolve the object interface
in an ad hoc way, by adding new methods and operators. For example, in
Scala, we can use an implicit conversion to add the multiplication
operator to pairs of integers, with the semantics of complex number
multiplication:

\begin{lstlisting-nobreak}
scala> (0, 1) * (0, 1)
res0: (Int, Int) = (-1, 0)
\end{lstlisting-nobreak}

% Interface okay, what about the object layout?
Unlike evolving the interface, there is no mechanism in modern
languages for evolving an object's encapsulated data as the programmer sees fit. The
encapsulated data format is assumed to be fixed, allowing the compiled
code to contain hard references to data, encoded according to a convention known as the
\emph{object layout}. For instance, methods encapsulated
by the generic pair class, such as |swap| and |toString|, rely on the
existence of two generic fields, erased to |Object|. This leads to
inefficient storage in the running example, as the integers need to be boxed, producing as many as
3 heap objects for each ``complex number'': the two boxed integers and
the pair container. What if, for a part of  our program, instead of the pair, we
concatenated the two 32-bit integers into a 64-bit long integer, that
would represent the ``complex number''? We could pass complex numbers by value,
completely sidestepping the need to allocate memory and to garbage-collect it
later. Additionally, what if we could also add
functionality, such as arithmetic operations, to our ad hoc complex
numbers, all without any heap allocation overhead? Finally, what parts of such a
transformation could be automated? %and what parts need domain-specific
%programmer intervention?

% Object layout transformations in dynamic language vms => slow (due to profiling and recompilation)
Object layout transformations are common in dynamic language virtual
machines, such as V8 and Truffle. These virtual machines profile
values at run-time and make optimistic assumptions about the shape of
objects. This allows them to improve the object layout
in the heap, at the cost of recompiling of all the code that references
the old object layout.
%However, in practice, the effort pays off.
If, later in the execution, the assumptions prove too optimistic, the
virtual machine needs to revert to the more general (and less
efficient) object layout, again recompiling all the code that contains
hard references to the optimized layout. As expected, this comes with
significant overheads. Thus, runtime decisions to change the
low-level layout are expensive (due to recompilation) and
have a global nature, affecting all code that assumes a certain layout.

%%\yannis{I don't think it's accurate to put the slowness down to that.}
%This makes dynamic languages several times slower than
%compiled, statically typed languages.

% Object layout transformations in statically typed languages => primitive unboxing, value classes and specialization
Since transforming the object layout at run-time is expensive, a
natural question to ask is whether we can leverage the
statically-typed nature of a programming language to optimize the
object layout during compilation? The answer is yes. Transformations such as ``class
specialization'' and ``value class inlining'' transform the object
layout in order to avoid the creation of heap objects. However, both
of these transformations take a global approach: when a class is
marked as specialized or as a value class (and assuming it satisfies the
semantic restrictions) it is transformed at its definition site. Later
on, this allows all references to the class, even in separately
compiled sources, to be optimized. On the other hand,
if a class is not marked at its definition site, retrofitting
specialization or the value class status is impossible, as it would
break many non-orthogonal language features, such as dynamic dispatch,
inheritance and generics.

% Object layout transformations in statically typed languages => not ad hoc
Therefore, although transformations in statically typed languages can
optimize the object layout, they do not meet the ad hoc criterion:
they cannot be retrofitted later, and they have a global, all-or-nothing
nature. For instance, in Scala, the generic pair class is specialized
but not marked as a value class. As a result, the representation is
not fully optimized, still requiring a heap object per pair. Even
worse, specialization and value class inlining are mutually exclusive,
making it impossible to optimally represent our ``complex numbers''
as values even if we had complete control over the Scala library.
Furthermore, a change in the data representation may be applicable for
specific parts of the client code, but might not make sense to apply globally.

% secret ingredient => the domain-specific information provided by the programmer
In our ``complex numbers'' abstraction, we only
use a fraction of the flexibility provided by the library tuples, and
yet we have to give up all the code optimality. Even worse, for our
limited domain, we are aware of a better representation, but
the only solution is to transform the code by hand, essentially having
to choose between an obfuscated or a slow version of the code. What is
missing is a largely automated and safe transformation that allows us
to use our domain-specific knowledge to mark a scope where the
``complex numbers'' can use the alternative object layout, effectively
specializing that part of our program.

In this paper we present such an automated transformation that allows programmers to safely change the data representation in limited, well-defined scopes that can include anything from expressions to method and class definitions. The transformation maintains strong correctness guarantees in terms of non-orthogonal language features, such as dynamic dispatch, inheritance and generics across separate compilations. To gain the most benefit, our approach uses the programmers' domain-specific knowledge of the transformed scope, allowing them to specify the exact alternative representation and the operations it should expose, while automating all the tedium involved in safely transforming the code and maintaining the outside interface.

In this way, the programmer is responsible for correctly stating (a) what the data representation transformation is and (b) to which program scope it applies. Our approach is then responsible for (1) automatically deciding when to apply the transformation and when to revert it, in order to ensure correct interchange between representations, (2) enriching the transformation with automatically generated bridge code that ensures correctness relative to overriding and dynamic dispatch and (3) persisting the necessary metadata to allow transformed program scopes in different source files and compilation runs to communicate using the optimized representation---a property we refer to as \emph{composability} in the following sections. Thus, our approach adheres to the design principle of separating the reusable, general and provably correct \emph{mechanism} from the programmer-defined \emph{policy}, which may contain incorrect decisions \cite{lampson-mechanism-policy}.

Our main contributions are:
\begin{itemize}
  \item Introducing the ad hoc data representation problem, which, to the best of our knowledge, has not been addressed at all in the literature (\S\ref{sec:problem});
  \item Presenting the extensions that allow global data representation transformations (\S\ref{sec:drt}) to be used as scoped programmer-driven transformations (\S\ref{sec:ildl});
  \item Implementing the approach presented as a Scala compiler plugin \cite{ildl-plugin} that allows programmers to express custom transformations (\S\ref{sec:impl}) and benchmarking the plugin on a broad spectrum of transformations, ranging from improving the data layout and encoding, to retrofitting specialization and value class status, and to collection deforestation \cite{wadler-deforestation}. These transformations produced  speedups between 1.8 and 24.5x on user programs (\S\ref{sec:benchmarks}).
\end{itemize}

% \vspace{0.5em}
% Using our approach and scoped programmer-driven transformations, we were able to cover the following use cases:
%
% \begin{itemize}
%   \item improving the data encoding and retrofitting the value class status for Scala containers (\S\ref{sec:benchmarks});
%   \item performing deforestation \cite{wadler-deforestation} and retrofitting specialization to Scala standard collections (\S\ref{sec:benchmarks});
%   \item converting arrays of structs to structs of arrays (\S\ref{sec:benchmarks});
%   \item using local and domain-specific knowledge to replace collections in a limited scope (\S\ref{sec:benchmarks}).
% \end{itemize}

%% \yannis{I recommend leaving this text out. I think it doesn't add
%% much. I integrated one important point (lampson) earlier.}
%The result of our work is an intuitive interface over an optimal,
%consistent and composable programmer-driven data representation
%transformation, where the composition works not only across source
%files but also across separate compilation
%(\S\ref{sec:ildl:signatures}). Furthermore, the transformation adheres
%to the principle of separating the reusable, general and provably
%correct mechanism from the programmer-driven policy, which may contain
%incorrect decisions \cite{lampson-mechanism-policy}
%(\S\ref{sec:ildl:discussion}). Overall, we feel the value brought by
%the transformation surpasses the sum of its individual components,
%opening new directions in compiler customization and programmer-driven
%transformation.

% The following section will describe the problem of ad hoc data representation transformations.
