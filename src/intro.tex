% Objects = encapsulated data + API. Extension methods = ad-hoc additions of the API

\begin{lstlisting-nobreak}
scala> (0f, 1f) * (0f, 1f)
res0: (Float, Float) = (-1.0, 0.0)
\end{lstlisting-nobreak}

% But how about allowing programmers to transform the encapsulated data in an ad-hoc way? In many cases, we end up with generic nested containers, which is an inefficient pattern, that produces extra objects ... For example, in the case of the tuple, the locations are ...
How about?

% Such types of transformations have typically been limited to dynamic language virtual machines, which are able to perform shape analysis and can bail out of transformations if necessary. But this is an expensive process, thus negating the benefits. Furthermore, good benchmark results are dependent on steady state, which essentially means the current set of assumptions is no longer being violated. What if, instead, we could use the programmer's domain-specific knowledge in the compiler?

% Arguably, the best solution would be to define a Complex value class, whose parameters can be completely unboxed and inlined, and replace all the uses of the tuple by this Complex class. Yet, unless the transformation occurs in isolation, or on a very small scale, reasoning about the complex interactions between overriding, generics, variance and top/bottom types is beyond.

% A more accessible solution would be to specialize tuple classes. Specialization duplicates and adapts classes, creating variants that can store unboxed values. But so far, retrofitting specialization has not been explored at all => all or nothing -- either a class is specialized from the beginning, or it is forever generic.

In this paper, we define the problem of ad-hoc data transformations and we offer a working solution. We also discuss the pitfalls we were able to retrofit specialization for functions by essentially performing ad-hoc object transformations in a compiler plugin, allowing show the pitfalls of retrofitting specialization to an existing library, essentially by performing


 * we state the problem of ad-hoc data transformations and the pitfalls that occur
 * in doing so, we build up our solution
 * we evaluate it, with surprisingly good results :)


Related work:
shape analysis, begone tagging












We can regard objects as encapsulating data and exposing an API. Extension methods in strongly typed object-oriented programming languages are mechanisms that allow the programmer to tweak an object's API. However, very little attention has been given to the problem of tweaking the data in an object while maintaining the API, particualrly for two reasons:

1. It's a very hard problem, usually because it requires compiler support (such as, for example, unboxing primitive types, specialization and value classes)
2. It is seen as a low-level transformation, to be done by virtual machine interpreters and


% - Although Scala and Java use the same mechanism for Functions, they are not compatible
% - However, a deeper question is interoperating between languages
% - Tension1: backwards compatibility
% - Generality of the problem
% - Bad solution, currently used in practice: binary incompatibility
% - Tension2: current DRT tecnhiques abund, but neither of them work incrementally -- they assume all repres are kwnown from day 1
% - Tension3: although we used the example of function representation
% - In this context, our paper makes the following contributions:
%    - define the problem
%    - incrementalizes existing DRT techniqes and explains the pitfalls
%    - show concrete numbers for a incremental DRT transformation based on function specialization, with 15x speedups


\section{Introduction}

Following the release of Java 8, both the language and the execution platform added mechanisms to support first-class functions. This also benefited the alternative languages targeting the Java Virtual Machine (JVM) execution platform, which used these mechanisms to improve their functions implementation, that, lacking platform support, was done in a clumsy and inefficient way. The following code snippet shows a method |foo| that accepts a function, written both in Java and Scala, one such alternative JVM language:

\begin{lstlisting-nobreak}
// Java:
Integer foo(Function<Integer, Integer> f) { ... }
foo(`(Integer x) -> x + 1`);
\end{lstlisting-nobreak}

\begin{lstlisting-nobreak}
// Scala:
def foo(f: Function1[Int, Int]): Int = { ... }
foo(`(x: Int) => x + 1`)
\end{lstlisting-nobreak}

Given the similarity of the syntax and knowing the implementations use the same underlying mechanisms, namely |MethodHandles| and the |invokeDynamic| instruction, it may be tempting to assume that the two languages can pass functions back and forth. However, this is not the case:

\begin{lstlisting-nobreak}
scala> val f = (x: Int) => x + 1
f: Function1[Int, Int] = <function1>

scala> import JavaExample.foo
import JavaExample.foo

scala> foo(f)
<console>:10: error: type mismatch;
 found   : scala.Function1[Int, Int]
 required: java.util.function.Function[Integer,Integer]
              foo(f)
                  ^
\end{lstlisting-nobreak}

Ideally, languages would expose a |Function| that abstracts over Java's |Function| and Scala's |Function1|, and uses the correct one depending on the context. Even more, since Scala benefits from function specialization \cite{iuli-thesis}, which allows calling functions without boxing their arguments and their return value, it should be favored over the Java representation when a choice can be made. \vlad{Other similar problems here.}

%% WE NEED A TRANSITION HERE!

% - Tension1: backwards compatibility
However, this raises to an important question: How should code compiled by previous versions of the Scala compiler, encoding functions as anonymous classes, be used from code compiled by the new versions of the Scala compiler, which use the Java 8 function encoding? Ideally, all the code should use the Java 8 representation, which is markedly better. But some libraries, such as |List| in our case, were already compiled with the earlier encoding, using anonymous classes. These would no longer be binary compatible, thus making it impossible to use them from code compiled using the new version of the compiler.

% - Generality
And this question is not limited to functions: the same problem pops up when trying to align the basic data containers across JVM languages. It would definitely be useful if developers could rely on a standard |Tuple|, |List| or |Stream| across all JVM languages. Finally, another example where this problem occurs is when using specialization techniques, such as specialization, miniboxing or the future Valhalla project \cite{}: how to bridge the gap between the otherwise incompatible

% - Bad solution, currently used in practice: binary incompatibility
The easy way out of the binary compatibility conundrum is to simply give up by declaring code compiled with the old and new versions of the compiler binary incompatible. In doing so, the responsibility is moved to programmers, who now need to make sure all their dependencies are compiled with the new version of the compiler, even if those dependencies are out of their control. This delays their work, in turn leading to slower adoption of the new version, and maybe even preventing it altogether, if the benefits do not overweight the adoption effort necessary.

% - Better solution: incremental data representation transformation
A better answer to the binary compatibility problem is to allow old and new bytecode to interoperate. Since old bytecode

% - Tension2: current DRT tecnhiques abund, but neither of them work incrementally -- they assume all repres are kwnown from day 1


% - Tension3: although we used the example of function representation

% - In this context, our paper makes the following contributions:
%    - define the problem
%    - incrementalizes existing DRT techniqes and explains the pitfalls
%    - show concrete numbers for a incremental DRT transformation based on function specialization, with 15x speedups


